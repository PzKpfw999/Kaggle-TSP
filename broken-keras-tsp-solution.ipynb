{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom matplotlib import pyplot as plt\n\nDATASET = 'tsp448/'\nTFRD_ROOT = '/kaggle/input/'+DATASET  #dataset path\nMODEL_ROOT = '/kaggle/input/models/'  #saved model path\nAUTOTUNE = tf.data.AUTOTUNE           # -1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver().connect()\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    tpu = None\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nif tpu:\n    from kaggle_secrets import UserSecretsClient\n    from kaggle_datasets import KaggleDatasets\n    user_secrets = UserSecretsClient()\n    user_credential = user_secrets.get_gcloud_credential()\n    user_secrets.set_tensorflow_credential(user_credential)\n    TFRD_ROOT = KaggleDatasets().get_gcs_path(DATASET[:-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TFRecordReader: \n    def __init__(self,filepath):\n        self.filepath = filepath\n\n    def decode_map(self):\n        feature_description = {\n            'img':tf.io.FixedLenFeature([], tf.string),\n            'label':tf.io.FixedLenFeature([], tf.float32)\n            }\n        def _decode_map(raw):\n            d = tf.io.parse_single_example(raw,feature_description)\n            img = tf.io.parse_tensor(d['img'],tf.float32)\n            img = tf.reshape(img,INPUT_SIZE)\n            return img,d['label']\n        return _decode_map\n    \n    def get_pipeline(self):\n        return tf.data.TFRecordDataset(self.filepath,'ZLIB',num_parallel_reads=AUTOTUNE\n                                                    ).map(self.decode_map(),AUTOTUNE)\n    \ndef augment_img_tensor_map(img,label):\n    return tf.image.rot90(img, np.random.choice([0,1,2,3])),label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_history(history):\n    plt.ylim([0,1000])\n    plt.plot(history.history['RMSE'])\n    plt.plot(history.history['val_RMSE'])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET_SIZE = (448,448)\nINPUT_SIZE = (448,448,3)\nMODEL_NAME = 'res50_5_23'\n\nif tpu:\n    BATCH_SIZE = 128 * strategy.num_replicas_in_sync\nelse:\n    BATCH_SIZE = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Input pipelines**\n\nIf tpu is used, there will be a lot of RAM, so cache all the data first.","metadata":{}},{"cell_type":"code","source":"pipeline = TFRecordReader(TFRD_ROOT+'/tsp_train.tfrecord').get_pipeline()\nval_pipeline = TFRecordReader(TFRD_ROOT+'/tsp_val.tfrecord').get_pipeline()\nif tpu:\n    pipeline = pipeline.cache(\n                            ).shuffle(4096, reshuffle_each_iteration=True\n                            ).map(augment_img_tensor_map,num_parallel_calls=AUTOTUNE,deterministic=False\n                            ).batch(BATCH_SIZE,drop_remainder=True\n                            ).prefetch(AUTOTUNE)\n    val_pipeline = val_pipeline.batch(82,drop_remainder=True\n                                ).cache(\n                                ).prefetch(AUTOTUNE)\nelse:\n    pipeline = pipeline.shuffle(256\n                            ).map(augment_img_tensor_map,num_parallel_calls=AUTOTUNE\n                            ).batch(BATCH_SIZE,drop_remainder=True\n                            ).prefetch(AUTOTUNE)\n    val_pipeline = val_pipeline.batch(BATCH_SIZE\n                                ).cache(\n                                ).prefetch(AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Callbacks**\n\ncheck_callback for save best model \n\nreduce_callback for reducing the learning rate when the loss did not drop","metadata":{}},{"cell_type":"code","source":"check_callback = keras.callbacks.ModelCheckpoint(MODEL_NAME+'.h5',save_best_only=True) \nreduce_callback = keras.callbacks.ReduceLROnPlateau('loss',factor=0.5,min_lr=1e-10,patience=6,verbose=1)\ncallbacks = [check_callback]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model**\n\nSince the features in the image is simple, we only use the 1,2,3 stage of res50 network for feature extraction. \n\nIt's much faster than the full res50 network, and did not loss many accuracy.\n\nUse Cov2D layer with 5x5 kernel size in first stage to get more details about the image.","metadata":{}},{"cell_type":"code","source":"def res50_5_23():\n    def identity_block(input_tensor, kernel_size, filters):\n        filters1, filters2, filters3 = filters\n\n        x = keras.layers.Conv2D(filters1, (1, 1),kernel_initializer='he_normal')(input_tensor)\n        x = keras.layers.BatchNormalization(axis=3)(x)\n        x = keras.layers.Activation('relu')(x)\n\n        x = keras.layers.Conv2D(filters2, kernel_size, padding='same', kernel_initializer='he_normal')(x)\n        x = keras.layers.BatchNormalization(axis=3)(x)\n        x = keras.layers.Activation('relu')(x)\n\n        x = keras.layers.Conv2D(filters3, (1, 1), kernel_initializer='he_normal')(x)\n        x = keras.layers.BatchNormalization(axis=3)(x)\n\n        x = keras.layers.add([x, input_tensor])\n        x = keras.layers.Activation('relu')(x)\n        return x\n\n    def conv_block(input_tensor,\n                kernel_size,\n                filters,\n                strides=(2, 2),padding='same'):\n        filters1, filters2, filters3 = filters\n\n        x = keras.layers.Conv2D(filters1, (1, 1), strides=strides,\n                        kernel_initializer='he_normal')(input_tensor)\n        x = keras.layers.BatchNormalization(axis=3)(x)\n        x = keras.layers.Activation('relu')(x)\n\n        x = keras.layers.Conv2D(filters2, kernel_size, padding=padding, kernel_initializer='he_normal')(x)\n        x = keras.layers.BatchNormalization(axis=3)(x)\n        x = keras.layers.Activation('relu')(x)\n\n        x = keras.layers.Conv2D(filters3, (1, 1), kernel_initializer='he_normal')(x)\n        x = keras.layers.BatchNormalization(axis=3)(x)\n\n        shortcut = keras.layers.Conv2D(filters3, (1, 1), strides=strides,kernel_initializer='he_normal')(input_tensor)\n        shortcut = keras.layers.BatchNormalization(axis=3)(shortcut)\n\n        x = keras.layers.add([x, shortcut])\n        x = keras.layers.Activation('relu')(x)\n        return x\n\n    input_layer = keras.layers.Input(INPUT_SIZE)\n    x = keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\")(input_layer)\n    \n    x = keras.layers.ZeroPadding2D(2)(x)\n    x = keras.layers.Conv2D(64,5,2,kernel_initializer='he_normal')(x)\n    x = keras.layers.BatchNormalization(axis=3)(x)\n    x = keras.layers.Activation('relu')(x)\n    x = keras.layers.MaxPooling2D()(x)\n\n    x = conv_block(x, 3, [64, 64, 256], strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256])\n    x = identity_block(x, 3, [64, 64, 256])\n\n    x = conv_block(x, 3, [128, 128, 512])\n    x = identity_block(x, 3, [128, 128, 512])\n    x = identity_block(x, 3, [128, 128, 512])\n    x = identity_block(x, 3, [128, 128, 512])\n\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dropout(0.4)(x)\n    x = keras.layers.Dense(512,activation=keras.activations.relu)(x)\n    x = keras.layers.Dropout(0.2)(x)\n    output = keras.layers.Dense(1)(x)\n\n    return keras.Model(inputs=input_layer,outputs=output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = res50_5_23()\n    model.compile(keras.optimizers.Adam(),loss=keras.losses.mse,metrics=[keras.metrics.RootMeanSquaredError(name='RMSE')])\nmodel.build(INPUT_SIZE) \nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(pipeline,epochs=150,validation_data=val_pipeline,callbacks=callbacks)#validation_data=val_pipeline\nshow_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Validation and Prediction**\n\nFollowing parts cannot run with TPU","metadata":{}},{"cell_type":"code","source":"def val_result(model,df:pd.DataFrame):\n    y_pred = calc_result(model,df)\n    y_true = df['distance'].to_numpy(np.float32)\n    rmse = keras.losses.mse(y_true,y_pred)**(1/2)\n    print(rmse)\n\ndef calc_result(model:keras.Model,df:pd.DataFrame):\n    arr_scale = []\n    def _gen():\n        for file in df['filename']:\n            img = load_jpeg(INPUT_ROOT+file)\n            img,scale = resize_img_tensor(img,TARGET_SIZE)\n            img = img/127.5 - 1\n            arr_scale.append(scale)\n            #print('\\r',file,end='')\n            yield img\n    \n    x = tf.data.Dataset.from_generator(_gen,tf.float32).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    y_pred = model.predict(x)\n    y_pred = np.asarray(y_pred,np.float32)\n    y_pred = y_pred.reshape((-1))\n    scale = np.asarray(arr_scale,np.float32)\n    y_pred = y_pred * scale\n    return y_pred\n\n\nINPUT_ROOT = '/kaggle/input/tsp-cv/' #img path\ntrain_csv_path = INPUT_ROOT+'train.csv'\ntest_csv_path = INPUT_ROOT+'test.csv'\ndf = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\n\nif os.path.exists(MODEL_NAME+'.h5'):\n    model = keras.models.load_model(MODEL_NAME+'.h5')\n\nval_result(model,df[:512])  #training set\nval_result(model,df[-512:]) #validation set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_y = calc_result(model,test_df)\ntest_df['distance'] = pred_y\ntest_df = test_df[['id','distance']]\ntest_df.to_csv('submit.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}